{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87c9ebd2-b1d4-428c-a0eb-2bbc6422f853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"D:\\\\Wine quality project\\\\winequality-red.csv\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8afc32-e628-4b2c-9179-b1392575b1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\red wine project\\.venv-gpu\\Lib\\site-packages\\xgboost\\compat.py:105: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns used:\n",
      "['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality']\n",
      "\n",
      "Label mapping (original -> encoded):\n",
      "{np.int64(3): np.int64(0), np.int64(4): np.int64(1), np.int64(5): np.int64(2), np.int64(6): np.int64(3), np.int64(7): np.int64(4), np.int64(8): np.int64(5)}\n",
      "\n",
      "Accuracy (keep all features): 0.6625\n",
      "\n",
      "Classification Report (encoded labels):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       0.50      0.09      0.15        11\n",
      "           2       0.72      0.72      0.72       136\n",
      "           3       0.62      0.69      0.65       128\n",
      "           4       0.71      0.60      0.65        40\n",
      "           5       0.33      0.33      0.33         3\n",
      "\n",
      "    accuracy                           0.66       320\n",
      "   macro avg       0.48      0.41      0.42       320\n",
      "weighted avg       0.66      0.66      0.65       320\n",
      "\n",
      "\n",
      "First 10 predicted wine qualities (original scale):\n",
      "[7 6 5 6 5 6 6 5 6 8]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Keep ALL features XGBoost pipeline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "print(\"Columns used:\")\n",
    "print(list(df.columns))\n",
    "print()\n",
    "\n",
    "\n",
    "if \"quality\" not in df.columns:\n",
    "    raise KeyError(\"No 'quality' column found in df. Check your earlier preprocessing.\")\n",
    "\n",
    "X = df.drop(columns=[\"quality\"])\n",
    "y_raw = df[\"quality\"]\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y_raw)\n",
    "\n",
    "print(\"Label mapping (original -> encoded):\")\n",
    "print(dict(zip(le.classes_, le.transform(le.classes_))))\n",
    "print()\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"xgb\", XGBClassifier(\n",
    "        eval_metric=\"mlogloss\",\n",
    "        random_state=42,\n",
    "        tree_method=\"hist\"  # comment out if causes issues\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(\"Accuracy (keep all features):\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report (encoded labels):\\n\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "y_pred_original = le.inverse_transform(y_pred)\n",
    "print(\"\\nFirst 10 predicted wine qualities (original scale):\")\n",
    "print(y_pred_original[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1737ab4",
   "metadata": {},
   "source": [
    "<span style=\"color:yellow; font-weight:bold;\">\n",
    "Removing all negative correlation features , based on heatmap\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7b3552-9aeb-4c80-8def-582971b8d1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping: {np.int64(3): np.int64(0), np.int64(4): np.int64(1), np.int64(5): np.int64(2), np.int64(6): np.int64(3), np.int64(7): np.int64(4), np.int64(8): np.int64(5)}\n",
      "Accuracy: 0.659375\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       0.33      0.09      0.14        11\n",
      "           2       0.69      0.78      0.73       136\n",
      "           3       0.64      0.66      0.65       128\n",
      "           4       0.68      0.47      0.56        40\n",
      "           5       0.20      0.33      0.25         3\n",
      "\n",
      "    accuracy                           0.66       320\n",
      "   macro avg       0.42      0.39      0.39       320\n",
      "weighted avg       0.65      0.66      0.65       320\n",
      "\n",
      "\n",
      "Predicted qualities (original scale):\n",
      "[7 6 5 6 6 5 6 5 5 8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\red wine project\\.venv-gpu\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "d:\\red wine project\\.venv-gpu\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "d:\\red wine project\\.venv-gpu\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Remove all negatively correlated features\n",
    "# Train XGBoost with preprocessing\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "neg_corr_features = [\n",
    "    \"volatile acidity\",\n",
    "    \"chlorides\",\n",
    "    \"density\",\n",
    "    \"pH\",\n",
    "    \"free sulfur dioxide\",\n",
    "    \"total sulfur dioxide\"\n",
    "]\n",
    "\n",
    "df2 = df.drop(columns=neg_corr_features, errors=\"ignore\")\n",
    "\n",
    "\n",
    "X = df2.drop(columns=[\"quality\"])\n",
    "y_raw = df2[\"quality\"]\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y_raw)\n",
    "\n",
    "print(\"Label mapping:\", dict(zip(le.classes_, le.transform(le.classes_))))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"xgb\", XGBClassifier(\n",
    "        eval_metric=\"mlogloss\",\n",
    "        random_state=42,\n",
    "        tree_method=\"hist\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "y_pred_original = le.inverse_transform(y_pred)\n",
    "print(\"\\nPredicted qualities (original scale):\")\n",
    "print(y_pred_original[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18afdeb6",
   "metadata": {},
   "source": [
    "<span style=\"color:red; font-weight:bold;\">\n",
    "XGBOOST and Imputer, 3 % Improvement\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871b7f6e-a7b0-44d0-869a-00aa6e5940e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping (original -> encoded):\n",
      "{np.int64(3): np.int64(0), np.int64(4): np.int64(1), np.int64(5): np.int64(2), np.int64(6): np.int64(3), np.int64(7): np.int64(4), np.int64(8): np.int64(5)}\n",
      "\n",
      "Accuracy: 0.65\n",
      "\n",
      "Classification Report (encoded labels):\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         2\n",
      "           1       0.40      0.18      0.25        11\n",
      "           2       0.70      0.76      0.73       136\n",
      "           3       0.63      0.63      0.63       128\n",
      "           4       0.58      0.53      0.55        40\n",
      "           5       0.50      0.33      0.40         3\n",
      "\n",
      "    accuracy                           0.65       320\n",
      "   macro avg       0.47      0.41      0.43       320\n",
      "weighted avg       0.64      0.65      0.64       320\n",
      "\n",
      "\n",
      "First 10 predicted wine qualities (original scale):\n",
      "[7 5 5 6 5 6 6 5 6 8]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cols_to_drop = [\"volatile acidity\", \"chlorides\", \"density\", \"total sulfur dioxide\"]\n",
    "df2 = df.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "\n",
    "\n",
    "if \"quality\" not in df2.columns:\n",
    "    raise KeyError(\"No 'quality' column found in df. Check your earlier preprocessing.\")\n",
    "\n",
    "X = df2.drop(columns=[\"quality\"])\n",
    "y_raw = df2[\"quality\"]\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y_raw)\n",
    "\n",
    "print(\"Label mapping (original -> encoded):\")\n",
    "print(dict(zip(le.classes_, le.transform(le.classes_))))\n",
    "print()\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"xgb\", XGBClassifier(\n",
    "        eval_metric=\"mlogloss\",\n",
    "        random_state=42,\n",
    "        tree_method=\"hist\"  \n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report (encoded labels):\\n\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "y_pred_original = le.inverse_transform(y_pred)\n",
    "print(\"\\nFirst 10 predicted wine qualities (original scale):\")\n",
    "print(y_pred_original[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6b659c",
   "metadata": {},
   "source": [
    "<span style=\"color:green; font-weight:bold;\">\n",
    "collapsing classes to 3 , low|medium|high . resulted in 20% improvement\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7b8992",
   "metadata": {},
   "source": [
    "Based on Kaggle website, it is advised to consider high and low classification for the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcde5862-afbb-4a96-851a-4c0a04618ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quality_3class\n",
      "1    1319\n",
      "2     217\n",
      "0      63\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#  Collapse classification to 3 classes\n",
    "df[\"quality_3class\"] = df[\"quality\"].apply(\n",
    "    lambda q: 0 if q <= 4 else (1 if q <= 6 else 2)\n",
    ")\n",
    "\n",
    "print(df[\"quality_3class\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47e3dd32-35f0-4afa-8d9b-180ec713de82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop([\"quality\", \"quality_3class\"], axis=1)\n",
    "y3 = df[\"quality_3class\"]\n",
    "\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(\n",
    "    X, y3,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7edf7a-87d3-4155-948b-ee791fadcdb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (1599, 12)\n",
      "Columns: ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality']\n",
      "\n",
      "6-class train/test shapes: (1279, 11) (320, 11)\n",
      "\n",
      "Fitting 6-class GridSearchCV...\n",
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n",
      "\n",
      "Best 6-class params: {'model__colsample_bytree': 0.8, 'model__learning_rate': 0.03, 'model__max_depth': 6, 'model__n_estimators': 200, 'model__subsample': 1.0}\n",
      "Best 6-class CV accuracy: 0.6622192169409903\n",
      "\n",
      "Clean best params for reuse: {'colsample_bytree': 0.8, 'learning_rate': 0.03, 'max_depth': 6, 'n_estimators': 200, 'subsample': 1.0}\n",
      "\n",
      "3-class value counts:\n",
      "quality_3class\n",
      "1    1319\n",
      "2     217\n",
      "0      63\n",
      "Name: count, dtype: int64\n",
      "\n",
      "3-class train/test shapes: (1279, 11) (320, 11)\n",
      "\n",
      "Fitting final 3-class XGBoost...\n",
      "\n",
      "=== 3-CLASS XGBOOST RESULTS (Low / Medium / High) ===\n",
      "3-class accuracy: 0.86875\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        13\n",
      "           1       0.89      0.95      0.92       264\n",
      "           2       0.72      0.60      0.66        43\n",
      "\n",
      "    accuracy                           0.87       320\n",
      "   macro avg       0.54      0.52      0.53       320\n",
      "weighted avg       0.83      0.87      0.85       320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import sys, subprocess\n",
    "\n",
    "\n",
    "try:\n",
    "    import xgboost\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"xgboost\"])\n",
    "    import xgboost\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "\n",
    "csv_path = r\"D:\\Wine quality project\\winequality-red.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(\"Data shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "\n",
    "X_full = df.drop(\"quality\", axis=1)\n",
    "y_full = df[\"quality\"]\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_full_enc = le.fit_transform(y_full)\n",
    "\n",
    "X_train6, X_test6, y_train6, y_test6 = train_test_split(\n",
    "    X_full, y_full_enc,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_full_enc\n",
    ")\n",
    "\n",
    "print(\"\\n6-class train/test shapes:\", X_train6.shape, X_test6.shape)\n",
    "\n",
    "#  GridSearchCV to find best XGB params on 6-class task\n",
    "six_class_pipe = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"model\", XGBClassifier(\n",
    "        objective=\"multi:softmax\",\n",
    "        num_class=len(le.classes_),\n",
    "        eval_metric=\"mlogloss\",\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"model__n_estimators\": [200, 400, 600],\n",
    "    \"model__max_depth\": [4, 6, 8],\n",
    "    \"model__learning_rate\": [0.03, 0.05],\n",
    "    \"model__subsample\": [0.8, 1.0],\n",
    "    \"model__colsample_bytree\": [0.8, 1.0],\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=six_class_pipe,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"accuracy\",\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(\"\\nFitting 6-class GridSearchCV...\")\n",
    "grid.fit(X_train6, y_train6)\n",
    "\n",
    "print(\"\\nBest 6-class params:\", grid.best_params_)\n",
    "print(\"Best 6-class CV accuracy:\", grid.best_score_)\n",
    "\n",
    "# Clean params dictionary (remove 'model__' prefix)\n",
    "best_params_clean = {k.replace(\"model__\", \"\"): v for k, v in grid.best_params_.items()}\n",
    "print(\"\\nClean best params for reuse:\", best_params_clean)\n",
    "\n",
    "#  Build 3-class target (Low / Medium / High) \n",
    "# Low: quality 3-4 >> 0\n",
    "# Medium: 5-6       >> 1\n",
    "# High: 7-8         >> 2\n",
    "df[\"quality_3class\"] = df[\"quality\"].apply(\n",
    "    lambda q: 0 if q <= 4 else (1 if q <= 6 else 2)\n",
    ")\n",
    "\n",
    "print(\"\\n3-class value counts:\")\n",
    "print(df[\"quality_3class\"].value_counts())\n",
    "\n",
    "X3 = df.drop([\"quality\", \"quality_3class\"], axis=1)\n",
    "y3 = df[\"quality_3class\"]\n",
    "\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(\n",
    "    X3, y3,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y3\n",
    ")\n",
    "\n",
    "print(\"\\n3-class train/test shapes:\", X_train3.shape, X_test3.shape)\n",
    "\n",
    "#  Train final 3-class XGBoost using best_params_clean ----\n",
    "# Impute\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "X_train3_imp = imputer.fit_transform(X_train3)\n",
    "X_test3_imp  = imputer.transform(X_test3)\n",
    "\n",
    "# Clean params so we can override objective/num_class/eval_metric/random_state\n",
    "xgb3_params = best_params_clean.copy()\n",
    "for k in [\"objective\", \"num_class\", \"eval_metric\", \"random_state\"]:\n",
    "    xgb3_params.pop(k, None)\n",
    "\n",
    "xgb3 = XGBClassifier(\n",
    "    **xgb3_params,\n",
    "    objective=\"multi:softmax\",\n",
    "    num_class=3,\n",
    "    eval_metric=\"mlogloss\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\nFitting final 3-class XGBoost...\")\n",
    "xgb3.fit(X_train3_imp, y_train3)\n",
    "\n",
    "#  Evaluate 3-class model \n",
    "y3_pred = xgb3.predict(X_test3_imp)\n",
    "\n",
    "print(\"\\n=== 3-CLASS XGBOOST RESULTS (Low / Medium / High) ===\")\n",
    "print(\"3-class accuracy:\", accuracy_score(y_test3, y3_pred))\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test3, y3_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b401fd68-049b-4b92-ab8a-810986ad2969",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
